{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "english_exp_names_1 = ['english', 'english_repeat_first', 'english_repeat_last', 'english_stopword']\n",
    "english_exp_names_3 = ['english_delete_5', 'english_delete_10', \\\n",
    "        'english_insert_5', 'english_insert_10', \\\n",
    "        'english_replace_5', 'english_replace_10', \\\n",
    "        'english_repeat_sentences', 'english_shuffle_sentences']\n",
    "\n",
    "exact_match = collections.defaultdict(list)\n",
    "rougeL = collections.defaultdict(list)\n",
    "classifications = ['answerability_classification', 'cause_effect_classification', 'coreference_resolution', \\\n",
    "    'dialogue_act_recognition', 'word_analogy', 'textual_entailment']\n",
    "non_classifications = ['data_to_text', 'grammar_error_correction', 'title_generation',\\\n",
    "    'keyword_tagging', 'overlap_extraction', 'question_rewriting']\n",
    "for english_exp_name in english_exp_names_1:\n",
    "    metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(english_exp_name), 'r')\n",
    "    metric = json.load(metric_file)\n",
    "\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metric['eval_exact_match_for_{}'.format(classification)])\n",
    "    exact_match['eval_exact_match'].append(metric['eval_exact_match'])\n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metric['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    rougeL['eval_rougeL'].append(metric['eval_rougeL'])\n",
    "\n",
    "metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(\"english\"), 'r')\n",
    "metric = json.load(metric_file)\n",
    "metric_names = metric.keys()\n",
    "\n",
    "for english_exp_name in english_exp_names_3:\n",
    "    metric_samples = dict.fromkeys(metric_names, [])\n",
    "    metric_means = dict.fromkeys(metric_names, [])\n",
    "    metric_stds = dict.fromkeys(metric_names, [])\n",
    "    for index in [1, 2, 3]:\n",
    "        _english_exp_name = english_exp_name + \"_{}\".format(index)\n",
    "\n",
    "        metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(_english_exp_name), 'r')\n",
    "        metric = json.load(metric_file)\n",
    "        for metric_name in metric_names:\n",
    "            metric_samples[metric_name].append(metric[metric_name])\n",
    "    \n",
    "    for metric_name in metric_names:\n",
    "        metric_means[metric_name] = np.mean(metric_samples[metric_name])\n",
    "    for metric_name in metric_names:\n",
    "        metric_stds[metric_name] = np.std(metric_samples[metric_name])\n",
    "\n",
    "    metrics = metric_means\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metrics['eval_exact_match_for_{}'.format(classification)])\n",
    "    \n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metrics['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    exact_match['eval_exact_match'].append(metrics['eval_exact_match'])\n",
    "    rougeL['eval_rougeL'].append(metrics['eval_rougeL'])\n",
    "english_rougeL = pd.DataFrame(rougeL)\n",
    "english_rougeL.index = english_exp_names_1 + english_exp_names_3\n",
    "english_exact_match = pd.DataFrame(exact_match)\n",
    "english_exact_match.index = english_exp_names_1 + english_exp_names_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_samples = dict.fromkeys(metric_names, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answerability_classification</th>\n",
       "      <th>cause_effect_classification</th>\n",
       "      <th>coreference_resolution</th>\n",
       "      <th>dialogue_act_recognition</th>\n",
       "      <th>word_analogy</th>\n",
       "      <th>textual_entailment</th>\n",
       "      <th>eval_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>english</th>\n",
       "      <td>59.384600</td>\n",
       "      <td>51.285700</td>\n",
       "      <td>49.428600</td>\n",
       "      <td>59.428600</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>45.708300</td>\n",
       "      <td>36.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_repeat_first</th>\n",
       "      <td>61.538500</td>\n",
       "      <td>48.571400</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>54.857100</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>46.083300</td>\n",
       "      <td>36.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_repeat_last</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>51.428600</td>\n",
       "      <td>47.785700</td>\n",
       "      <td>48.285700</td>\n",
       "      <td>49.375000</td>\n",
       "      <td>50.166700</td>\n",
       "      <td>36.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_stopword</th>\n",
       "      <td>61.153800</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>49.285700</td>\n",
       "      <td>48.625000</td>\n",
       "      <td>44.833300</td>\n",
       "      <td>36.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_delete_5</th>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "      <td>84.318124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_delete_10</th>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "      <td>84.261885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_insert_5</th>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "      <td>84.719801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_insert_10</th>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "      <td>84.618102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_replace_5</th>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "      <td>84.689185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_replace_10</th>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "      <td>84.302596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_repeat_sentences</th>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "      <td>85.254613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_shuffle_sentences</th>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "      <td>84.842538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           answerability_classification  \\\n",
       "english                                       59.384600   \n",
       "english_repeat_first                          61.538500   \n",
       "english_repeat_last                           60.000000   \n",
       "english_stopword                              61.153800   \n",
       "english_delete_5                              84.318124   \n",
       "english_delete_10                             84.261885   \n",
       "english_insert_5                              84.719801   \n",
       "english_insert_10                             84.618102   \n",
       "english_replace_5                             84.689185   \n",
       "english_replace_10                            84.302596   \n",
       "english_repeat_sentences                      85.254613   \n",
       "english_shuffle_sentences                     84.842538   \n",
       "\n",
       "                           cause_effect_classification  \\\n",
       "english                                      51.285700   \n",
       "english_repeat_first                         48.571400   \n",
       "english_repeat_last                          51.428600   \n",
       "english_stopword                             49.000000   \n",
       "english_delete_5                             84.318124   \n",
       "english_delete_10                            84.261885   \n",
       "english_insert_5                             84.719801   \n",
       "english_insert_10                            84.618102   \n",
       "english_replace_5                            84.689185   \n",
       "english_replace_10                           84.302596   \n",
       "english_repeat_sentences                     85.254613   \n",
       "english_shuffle_sentences                    84.842538   \n",
       "\n",
       "                           coreference_resolution  dialogue_act_recognition  \\\n",
       "english                                 49.428600                 59.428600   \n",
       "english_repeat_first                    48.500000                 54.857100   \n",
       "english_repeat_last                     47.785700                 48.285700   \n",
       "english_stopword                        50.000000                 49.285700   \n",
       "english_delete_5                        84.318124                 84.318124   \n",
       "english_delete_10                       84.261885                 84.261885   \n",
       "english_insert_5                        84.719801                 84.719801   \n",
       "english_insert_10                       84.618102                 84.618102   \n",
       "english_replace_5                       84.689185                 84.689185   \n",
       "english_replace_10                      84.302596                 84.302596   \n",
       "english_repeat_sentences                85.254613                 85.254613   \n",
       "english_shuffle_sentences               84.842538                 84.842538   \n",
       "\n",
       "                           word_analogy  textual_entailment  eval_exact_match  \n",
       "english                       52.250000           45.708300         36.841700  \n",
       "english_repeat_first          50.000000           46.083300         36.630000  \n",
       "english_repeat_last           49.375000           50.166700         36.596100  \n",
       "english_stopword              48.625000           44.833300         36.105000  \n",
       "english_delete_5              84.318124           84.318124         84.318124  \n",
       "english_delete_10             84.261885           84.261885         84.261885  \n",
       "english_insert_5              84.719801           84.719801         84.719801  \n",
       "english_insert_10             84.618102           84.618102         84.618102  \n",
       "english_replace_5             84.689185           84.689185         84.689185  \n",
       "english_replace_10            84.302596           84.302596         84.302596  \n",
       "english_repeat_sentences      85.254613           85.254613         85.254613  \n",
       "english_shuffle_sentences     84.842538           84.842538         84.842538  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "xlingual_exp_names_1 = ['xlingual', 'xlingual_repeat_first', 'xlingual_repeat_last', 'xlingual_stopword']\n",
    "xlingual_exp_names_3 = ['xlingual_delete_5', 'xlingual_delete_10', \\\n",
    "        'xlingual_insert_5', 'xlingual_insert_10', \\\n",
    "        'xlingual_replace_5', 'xlingual_replace_10', \\\n",
    "        'xlingual_repeat_sentences', 'xlingual_shuffle_sentences']\n",
    "exact_match = collections.defaultdict(list)\n",
    "rougeL = collections.defaultdict(list)\n",
    "classifications = ['answerability_classification', 'cause_effect_classification', 'textual_entailment']\n",
    "non_classifications = ['title_generation']\n",
    "for xlingual_exp_name in xlingual_exp_names_1:\n",
    "    metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(xlingual_exp_name), 'r')\n",
    "    metric = json.load(metric_file)\n",
    "\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metric['eval_exact_match_for_{}'.format(classification)])\n",
    "    exact_match['eval_exact_match'].append(metric['eval_exact_match'])\n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metric['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    rougeL['eval_rougeL'].append(metric['eval_rougeL'])\n",
    "\n",
    "metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(\"xlingual\"), 'r')\n",
    "metric = json.load(metric_file)\n",
    "metric_names = metric.keys()\n",
    "\n",
    "for xlingual_exp_name in xlingual_exp_names_3:\n",
    "    metric_samples = dict.fromkeys(metric_names, [])\n",
    "    metric_means = dict.fromkeys(metric_names, [])\n",
    "    metric_stds = dict.fromkeys(metric_names, [])\n",
    "    for index in [1, 2, 3]:\n",
    "        _xlingual_exp_name = xlingual_exp_name + \"_{}\".format(index)\n",
    "\n",
    "        metric_file = open('/home/gujiashe/Tk-Instruct/output_{}/all_results.json'.format(_xlingual_exp_name), 'r')\n",
    "        metric = json.load(metric_file)\n",
    "        for metric_name in metric_names:\n",
    "            metric_samples[metric_name].append(metric[metric_name])\n",
    "    for metric_name in metric_names:\n",
    "        metric_means[metric_name] = np.mean(metric_samples[metric_name])\n",
    "    for metric_name in metric_names:\n",
    "        metric_stds[metric_name] = np.std(metric_samples[metric_name])\n",
    "\n",
    "    metrics = metric_stds\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metrics['eval_exact_match_for_{}'.format(classification)])\n",
    "    \n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metrics['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    exact_match['eval_exact_match'].append(metrics['eval_exact_match'])\n",
    "    rougeL['eval_rougeL'].append(metrics['eval_rougeL'])\n",
    "xlingual_rougeL = pd.DataFrame(rougeL)\n",
    "xlingual_rougeL.index = xlingual_exp_names_1 + xlingual_exp_names_3\n",
    "xlingual_exact_match = pd.DataFrame(exact_match)\n",
    "xlingual_exact_match.index = xlingual_exp_names_1 + xlingual_exp_names_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'english_exact_match': english_exact_match, 'english_rougeL': english_rougeL, \\\n",
    "    'xlingual_exact_match': xlingual_exact_match, 'xlingual_rougeL': xlingual_rougeL}\n",
    "for metric_name, metric_var in metrics.items():\n",
    "    metric_var.to_csv(metric_name+'_stds.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test tk_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/tk-instruct-3b-def\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/tk-instruct-3b-def\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "        # \"Definition: return the currency of the country. Now complete the following example - Input: China. Output:\", \n",
    "        \"فتوسنتز فرایندی زیست‌شیمیایی است که در آن، انرژی نورانی خورشید توسط گیاهان و برخی از باکتری‌ها به انرژی شیمیایی ذخیره‌شده در مواد غذایی آن‌ها تبدیل می‌شود. کمابیش همهٔ جانداران روی زمین به آن وابسته‌اند. در عمل فتوسنتز، اندام‌هایی مانند برگ که دارای سبزینه هستند، کربن دی‌اکسید، آب و نور را جذب کرده و به کلروپلاست می‌رسانند. طی واکنش‌هایی که درون کلروپلاست انجام می‌گیرد، این مواد به اکسیژن و کربوهیدرات‌ها تبدیل می‌شوند. همه اکسیژن کنونی موجود بر روی زمین، فراوردهٔ فتوسنتز است. برخی از کربوهیدرات‌های مهم تولیدشده مانند گلوکز، می‌توانند به دیگر مواد آلی، لیپیدها، نشاسته، سلولز و پروتئین تبدیل شوند که برای تبدیل‌شدن به پروتئین، نیاز به نیتروژن دارند. ژان باپتیست ون هلمونت، یکی از نخستین آزمایش‌های مربوط به فتوسنتز را انجام داد. همه بخش‌های سبزرنگ گیاه، قادر به انجام عمل فتوسنتز هستند. مادهٔ سبز موجود در گیاهان که سبزینه یا کلروفیل نام دارد، آغازکنندهٔ واکنش‌های فتوسنتز است. فتوسنتز در اندام‌هایی که فاقد سبزینه هستند، انجام نمی‌گیرد.\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "output = model.generate(input_ids, max_length=10)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load ni_dataset_crud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "raw_datasets = load_dataset(\n",
    "    \"src/ni_dataset_crud.py\", \n",
    "    data_dir='data/splits/default', \n",
    "    task_dir='data/tasks', \n",
    "    cache_dir='./cache/',\n",
    "    max_num_instances_per_task=100,\n",
    "    max_num_instances_per_eval_task=100,\n",
    "    download_mode = 'reuse_cache_if_exists'\n",
    ")\n",
    "print(raw_datasets['validation'][101]['Definition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['In this task your given two statements in Estonian. You must judge whether the second sentence is the cause or effect of the first one. Label the instances as \"cause\" or \"effect\" based on your judgment. The sentences are separated by a newline character.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_). The word between ** ** is the target name. The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Generate an appropriate title for the given text. The generated title must be short and include the main topic of the text. \\\n",
    "The preferred titles are under fifteen words.\"\n",
    "\"Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_). \\\n",
    "The word between ** ** is the target name. \\\n",
    "The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.en = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    def delete_ratio(self, Definition, ratio=0.1):\n",
    "\n",
    "        splited_definition = Definition.split()\n",
    "        index = [i for i in range(len(splited_definition))]\n",
    "        index = random.sample(index, int(len(splited_definition)*ratio))\n",
    "        index.sort()\n",
    "        Definition_crud = \" \".join([splited_definition[i] for i in index])\n",
    "        return Definition_crud\n",
    "\n",
    "    def delete_num(self, Definition, num=5):\n",
    "\n",
    "        splited_definition = Definition.split()\n",
    "        index = [i for i in range(len(splited_definition))]\n",
    "        index = random.sample(index, len(splited_definition)-num)\n",
    "        index.sort()\n",
    "        Definition_crud = \" \".join([splited_definition[i] for i in index])\n",
    "        return Definition_crud\n",
    "    def delete_stopwords(self, Definition):\n",
    "\n",
    "        #loading the english language small model of spacy\n",
    "        \n",
    "        stopwords = self.en.Defaults.stop_words\n",
    "        lst=[]\n",
    "        for token in Definition.split():\n",
    "            if token.lower() not in stopwords:    #checking whether the word is not \n",
    "                lst.append(token)                    #present in the stopword list.\n",
    "        Definition_crud = \" \".join(lst)     \n",
    "        return Definition_crud\n",
    "\n",
    "    def insert_mask(self, Definition, num_mask=5):\n",
    "\n",
    "        token_list = Definition.split()\n",
    "\n",
    "\n",
    "        num = 0\n",
    "        while num < num_mask:\n",
    "            num += 1\n",
    "            insert_position = random.randint(1, len(token_list) - 1)\n",
    "            token_list.insert(insert_position, '[MASK]')\n",
    "    \n",
    "\n",
    "        input_txt = ' '.join(token_list)\n",
    "\n",
    "        inputs = self.tokenizer(input_txt, return_tensors='pt')\n",
    "\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0].numpy()\n",
    "        if input_ids.shape[0]>512:\n",
    "            return Definition\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "        _, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "        \n",
    "        for k in range(1):\n",
    "            predicted_index = [sorted_idx[i, k].item() for i in range(0, len(predictions[0])-1)]\n",
    "            predicted_token = []\n",
    "            for x in range(1, len(predictions[0])-1):\n",
    "                if input_ids[x] == 103:\n",
    "                    predicted_token.append(self.tokenizer.convert_ids_to_tokens([predicted_index[x]])[0])\n",
    "        copy_token = predicted_token.copy()\n",
    "        token_list_copy = token_list.copy()\n",
    "        for i, token in enumerate(token_list):\n",
    "            if token == '[MASK]':\n",
    "                if len(predicted_token)==0:\n",
    "                    print(Definition)\n",
    "                    print(token_list_copy)\n",
    "                    print(copy_token)\n",
    "                token_list[i] = predicted_token.pop(0)\n",
    "        final_tokens = []\n",
    "        for token in token_list:\n",
    "            if token.startswith('##'):\n",
    "                final_tokens[-1] = final_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                final_tokens.append(token)\n",
    "\n",
    "        return \" \".join(final_tokens)\n",
    "        \n",
    "    def shuffle_sentences(self, Definition):\n",
    "\n",
    "        doc = self.en(Definition)\n",
    "        sents = list(map(str, doc.sents))\n",
    "        random.shuffle(sents)\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    def repeat_sentences(self, Definition, index = None):\n",
    "        doc = self.en(Definition)\n",
    "        sents = list(map(str, doc.sents))\n",
    "        if  None==index:\n",
    "            index = random.randint(0, len(sents)-1)\n",
    "        sents = sents[:index] + [sents[index]] + sents[index:]\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    def replace_mask(self, Definition, ratio=0.1):\n",
    "\n",
    "        inputs = self.tokenizer(Definition, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "\n",
    "        index = [i for i in range(len(input_ids))]\n",
    "        index = random.sample(index, int(len(input_ids)*ratio))\n",
    "        index.sort()\n",
    "        for i in index:\n",
    "            input_ids[i] = 103\n",
    "\n",
    "\n",
    "        if len(input_ids)>512:\n",
    "            return Definition\n",
    "        inputs['input_ids'][0] = input_ids\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "        _, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "        \n",
    "\n",
    "        for k in range(1):\n",
    "            predicted_index = [sorted_idx[i, k].item() for i in range(0, len(predictions[0])-1)]\n",
    "            for x in range(1, len(predictions[0])-1):\n",
    "                if input_ids[x] == 103:\n",
    "                    input_ids[x] = predicted_index[x]\n",
    "\n",
    "        return self.tokenizer.decode(input_ids[1: -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the passage and find the corresponding pronoun for the given name. Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_).     The word between ** ** is the target name.         The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_). \\\n",
    "    The word between ** ** is the target name. \\\n",
    "        The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\"\n",
    "\n",
    "data_augmentation = DataAugmentation()\n",
    "data_crud= data_augmentation.repeat_sentences(dataset, index = 0)\n",
    "print(data_crud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 11,  3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2, 3])\n",
    "np.insert(a, 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 0 1 8'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.decode(10434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'a', 'beautiful', 'day', ',', 'perfect', 'for', 'a', 'trip', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.tokenize(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    " \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "input_txt = data_crud\n",
    "inputs = tokenizer(input_txt, return_tensors='pt', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    " \n",
    "outputs = model(**inputs)\n",
    "predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, 119, 1541, 1108, 1216, 170, 2712, 1285, 117, 1103, 3264, 1285, 1111, 170, 1263, 3868, 119]\n",
      "['It', 'really', 'was', 'such', 'a', 'beautiful', 'day', ',', 'the', 'perfect', 'day', 'for', 'a', 'long', 'trip', '.']\n"
     ]
    }
   ],
   "source": [
    "_, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "input_ids = inputs['input_ids'][0].numpy()\n",
    "for k in range(1):\n",
    "    predicted_index = [sorted_idx[i, k].item() for i in range(0, len(predictions[0])-1)]\n",
    "    print(predicted_index)\n",
    "    predicted_token = []\n",
    "    for x in range(1, len(predictions[0])-1):\n",
    "        if input_ids[x] == 103:\n",
    "            predicted_token.append(tokenizer.convert_ids_to_tokens([predicted_index[x]])[0])\n",
    "        else:\n",
    "            predicted_token.append(tokenizer.convert_ids_to_tokens([input_ids[x]])[0])\n",
    "    print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It really was such a beautiful day , the perfect day for a long trip .\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for token in predicted_token:\n",
    "    if token.startswith('##'):\n",
    "        tokens[-1] = tokens[-1] + token[2:]\n",
    "    else:\n",
    "        tokens.append(token)\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_). The word between ** ** is the target name. The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\",\n",
       " 'Read the passage and find the corresponding pronoun for the given name.',\n",
       " 'The pronoun should match the given blank(_).',\n",
       " 'The word between ** ** is the target name.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(map(str, doc.sents))\n",
    "random.shuffle(sents)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the passage and find the corresponding pronoun for the given name.\n",
      "The pronoun should match the given blank(_).\n",
      "The word between ** ** is the target name.\n",
      "The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\n"
     ]
    }
   ],
   "source": [
    "for sent in list(doc.sents):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc2c0cd89e4ba02e191b1e7889aaaf3c8919053db7e24df61ffe975264e317e1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
