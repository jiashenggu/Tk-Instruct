{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['data_to_text', 'grammar_error_correction', 'title_generation',\n",
      "       'keyword_tagging', 'overlap_extraction', 'question_rewriting',\n",
      "       'eval_rougeL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "english_exp_names_1 = []\n",
    "# english_exp_names_3 = ['english', 'english_stopword', 'english_delete_5', 'english_delete_10', \\\n",
    "#         'english_insert_5', 'english_insert_10', \\\n",
    "#         'xlingualreplace_5', 'english_replace_10', 'xlingualshuffle_words', \\\n",
    "#         'english_repeat_sentences', 'english_shuffle_sentences', 'english_shuffle_instruction']\n",
    "english_exp_names_3 = ['english_original', 'english_delete_stopwords', 'english_delete_10_words', \\\n",
    "        'english_insert_10_words', 'english_replace_10_words', 'english_shuffle_words', \\\n",
    "        'english_repeat_sentences', 'english_shuffle_sentences', 'english_shuffle_instructions']\n",
    "exact_match = collections.defaultdict(list)\n",
    "rougeL = collections.defaultdict(list)\n",
    "classifications = ['answerability_classification', 'cause_effect_classification', 'coreference_resolution', \\\n",
    "    'dialogue_act_recognition', 'word_analogy', 'textual_entailment']\n",
    "non_classifications = ['data_to_text', 'grammar_error_correction', 'title_generation',\\\n",
    "    'keyword_tagging', 'overlap_extraction', 'question_rewriting']\n",
    "for english_exp_name in english_exp_names_1:\n",
    "    metric_file = open('./tk_outputs/output_{}/all_results.json'.format(english_exp_name), 'r')\n",
    "    metric = json.load(metric_file)\n",
    "\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metric['eval_exact_match_for_{}'.format(classification)])\n",
    "    exact_match['eval_exact_match'].append(metric['eval_exact_match'])\n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metric['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    rougeL['eval_rougeL'].append(metric['eval_rougeL'])\n",
    "\n",
    "metric_file = open('./tk_outputs/output_{}/all_results.json'.format(\"english\"), 'r')\n",
    "metric = json.load(metric_file)\n",
    "metric_names = metric.keys()\n",
    "def create_dict(keys):\n",
    "    d = {}\n",
    "    for key in keys:\n",
    "        d[key] = []\n",
    "    return d    \n",
    "for english_exp_name in english_exp_names_3:\n",
    "    metric_samples = create_dict(metric_names)\n",
    "    metric_means = create_dict(metric_names)\n",
    "    metric_stds = create_dict(metric_names)\n",
    "    for index in range(4, 7):\n",
    "        _english_exp_name = english_exp_name + \"_{}\".format(index)\n",
    "\n",
    "        metric_file = open('./tk_outputs/output_{}/all_results.json'.format(_english_exp_name), 'r')\n",
    "        metric = json.load(metric_file)\n",
    "        for metric_name in metric_names:\n",
    "            metric_samples[metric_name].append(metric[metric_name])\n",
    "    \n",
    "    for metric_name in metric_names:\n",
    "        metric_means[metric_name] = round(np.mean(metric_samples[metric_name]), 1)\n",
    "    for metric_name in metric_names:\n",
    "        metric_stds[metric_name] = round(np.std(metric_samples[metric_name]), 1)\n",
    "\n",
    "    # metrics = metric_means\n",
    "\n",
    "    for classification in classifications:\n",
    "        metric_mean = metric_means['eval_exact_match_for_{}'.format(classification)]\n",
    "        metric_std = metric_stds['eval_exact_match_for_{}'.format(classification)]\n",
    "        exact_match[classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    \n",
    "    for non_classification in non_classifications:\n",
    "        metric_mean = metric_means['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        metric_std = metric_stds['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        rougeL[non_classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    exact_match['eval_exact_match'].append(str(metric_means['eval_exact_match']) + \"\\u00B1\" + str(metric_stds['eval_exact_match']))\n",
    "    rougeL['eval_rougeL'].append(str(metric_means['eval_rougeL']) + \"\\u00B1\" + str(metric_stds['eval_rougeL']))\n",
    "english_rougeL = pd.DataFrame(rougeL)\n",
    "english_rougeL.index = english_exp_names_1 + english_exp_names_3\n",
    "english_exact_match = pd.DataFrame(exact_match)\n",
    "english_exact_match.index = english_exp_names_1 + english_exp_names_3\n",
    "\n",
    "def generate_new_columns(columns):\n",
    "    new_columns = []\n",
    "    for c in columns:\n",
    "        new_column = []\n",
    "        for first in c.split('_'):\n",
    "            new_column.append(first[0].upper())\n",
    "        new_columns.append(\"\".join(new_column))\n",
    "    return new_columns\n",
    "print(english_rougeL.columns)\n",
    "english_exact_match.columns = generate_new_columns(english_exact_match.columns)\n",
    "english_rougeL.columns = generate_new_columns(english_rougeL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "{} &        AC &       CEC &        CR &       DAR &        WA &        TE \\\\\n",
      "\\midrule\n",
      "english\\_original             &  49.8±0.6 &  50.0±0.8 &  36.1±0.5 &  43.2±0.8 &  19.7±0.7 &  40.1±0.8 \\\\\n",
      "english\\_delete\\_stopwords     &  44.2±0.4 &  45.0±1.2 &  32.3±0.7 &  28.7±0.9 &   7.6±0.1 &  31.6±0.5 \\\\\n",
      "english\\_delete\\_10\\_words      &  45.3±3.3 &  40.8±4.3 &  32.5±0.4 &  34.0±2.6 &  18.6±2.6 &  34.9±3.1 \\\\\n",
      "english\\_insert\\_10\\_words      &  44.5±1.9 &  40.3±4.2 &  34.0±2.0 &  38.9±1.2 &  18.5±1.2 &  35.1±0.4 \\\\\n",
      "english\\_replace\\_10\\_words     &  48.7±1.0 &  45.9±1.6 &  31.3±1.9 &  35.9±1.8 &  18.2±0.6 &  33.8±1.7 \\\\\n",
      "english\\_shuffle\\_words        &  20.1±2.7 &   4.2±2.5 &  18.1±2.0 &   7.8±1.6 &  14.2±2.0 &   8.6±0.5 \\\\\n",
      "english\\_repeat\\_sentences     &  51.1±0.8 &  46.4±1.0 &  34.8±1.0 &  37.6±4.0 &  16.4±2.3 &  41.3±1.4 \\\\\n",
      "english\\_shuffle\\_sentences    &  49.9±1.2 &  46.6±1.3 &  34.0±1.0 &  42.4±0.1 &  16.4±1.5 &  40.0±0.9 \\\\\n",
      "english\\_shuffle\\_instructions &   2.4±2.6 &   3.0±3.9 &   4.2±2.2 &   2.7±3.7 &   1.8±1.3 &   1.2±1.5 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "{} &       DTT &       GEC &        TG &         KT &        OE &        QR \\\\\n",
      "\\midrule\n",
      "english\\_original             &  31.6±0.4 &  83.1±1.4 &  33.8±0.6 &   56.3±1.7 &  24.4±2.8 &  48.2±0.4 \\\\\n",
      "english\\_delete\\_stopwords     &  27.8±0.4 &  83.3±0.6 &  31.5±0.3 &   49.9±1.1 &  22.6±0.6 &  37.2±0.2 \\\\\n",
      "english\\_delete\\_10\\_words      &  29.9±1.3 &  78.2±8.8 &  31.0±0.8 &   49.7±4.6 &  24.0±0.8 &  41.0±2.3 \\\\\n",
      "english\\_insert\\_10\\_words      &  30.9±0.3 &  81.2±1.2 &  33.2±0.7 &   54.2±1.1 &  25.3±0.5 &  46.8±0.4 \\\\\n",
      "english\\_replace\\_10\\_words     &  31.3±0.9 &  82.6±0.7 &  30.9±0.7 &   53.5±2.8 &  26.9±1.2 &  42.9±3.6 \\\\\n",
      "english\\_shuffle\\_words        &  20.7±2.6 &  71.1±5.2 &  21.9±0.5 &   33.9±2.0 &  22.9±1.0 &  24.3±1.3 \\\\\n",
      "english\\_repeat\\_sentences     &  33.1±0.5 &  83.5±0.2 &  31.6±0.6 &   52.0±3.1 &  26.9±0.6 &  52.0±0.4 \\\\\n",
      "english\\_shuffle\\_sentences    &  31.1±0.2 &  83.6±2.3 &  33.9±0.5 &   55.1±1.2 &  25.8±0.6 &  44.6±2.3 \\\\\n",
      "english\\_shuffle\\_instructions &   9.9±2.2 &   7.2±8.4 &  10.1±1.4 &  18.8±11.3 &  15.0±6.0 &  16.7±5.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54076/2916011428.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(english_exact_match.iloc[:, :-1].to_latex())\n",
      "/tmp/ipykernel_54076/2916011428.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(english_rougeL.iloc[:, :-1].to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(english_exact_match.iloc[:, :-1].to_latex())\n",
    "print(english_rougeL.iloc[:, :-1].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "xlingual_exp_names_1 = []\n",
    "# xlingual_exp_names_3 = ['xlingual', 'xlingual_stopword', 'xlingual_delete_5', 'xlingual_delete_10', \\\n",
    "#         'xlingual_insert_5', 'xlingual_insert_10', \\\n",
    "#         'xlingual_replace_5', 'xlingual_replace_10', \\\n",
    "#         'xlingual_repeat_sentences', 'xlingual_shuffle_sentences', 'xlingual_shuffle_words', 'xlingual_shuffle_instruction']\n",
    "xlingual_exp_names_3 = ['xlingual_original', 'xlingual_delete_stopwords', 'xlingual_delete_10_words', \\\n",
    "        'xlingual_insert_10_words', 'xlingual_replace_10_words', 'xlingual_shuffle_words', \\\n",
    "        'xlingual_repeat_sentences', 'xlingual_shuffle_sentences', 'xlingual_shuffle_instructions']\n",
    "exact_match = collections.defaultdict(list)\n",
    "rougeL = collections.defaultdict(list)\n",
    "classifications = ['answerability_classification', 'cause_effect_classification', 'textual_entailment']\n",
    "non_classifications = ['title_generation']\n",
    "for xlingual_exp_name in xlingual_exp_names_1:\n",
    "    metric_file = open('./tk_outputs/output_{}/all_results.json'.format(xlingual_exp_name), 'r')\n",
    "    metric = json.load(metric_file)\n",
    "\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metric['eval_exact_match_for_{}'.format(classification)])\n",
    "    exact_match['eval_exact_match'].append(metric['eval_exact_match'])\n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metric['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    rougeL['eval_rougeL'].append(metric['eval_rougeL'])\n",
    "\n",
    "metric_file = open('./tk_outputs/output_{}/all_results.json'.format(\"xlingual\"), 'r')\n",
    "metric = json.load(metric_file)\n",
    "metric_names = metric.keys()\n",
    "def create_dict(keys):\n",
    "    d = {}\n",
    "    for key in keys:\n",
    "        d[key] = []\n",
    "    return d    \n",
    "for xlingual_exp_name in xlingual_exp_names_3:\n",
    "    metric_samples = create_dict(metric_names)\n",
    "    metric_means = create_dict(metric_names)\n",
    "    metric_stds = create_dict(metric_names)\n",
    "    for index in range(4, 7):\n",
    "        _xlingual_exp_name = xlingual_exp_name + \"_{}\".format(index)\n",
    "\n",
    "        metric_file = open('./tk_outputs/output_{}/all_results.json'.format(_xlingual_exp_name), 'r')\n",
    "        metric = json.load(metric_file)\n",
    "        for metric_name in metric_names:\n",
    "            metric_samples[metric_name].append(metric[metric_name])\n",
    "    for metric_name in metric_names:\n",
    "        metric_means[metric_name] = round(np.mean(metric_samples[metric_name]), 1)\n",
    "    for metric_name in metric_names:\n",
    "        metric_stds[metric_name] = round(np.std(metric_samples[metric_name]), 1)\n",
    "\n",
    "    # metrics = metric_means\n",
    "\n",
    "    for classification in classifications:\n",
    "        metric_mean = metric_means['eval_exact_match_for_{}'.format(classification)]\n",
    "        metric_std = metric_stds['eval_exact_match_for_{}'.format(classification)]\n",
    "        exact_match[classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    \n",
    "    for non_classification in non_classifications:\n",
    "        metric_mean = metric_means['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        metric_std = metric_stds['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        rougeL[non_classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    exact_match['eval_exact_match'].append(str(metric_means['eval_exact_match']) + \"\\u00B1\" + str(metric_stds['eval_exact_match']))\n",
    "    rougeL['eval_rougeL'].append(str(metric_means['eval_rougeL']) + \"\\u00B1\" + str(metric_stds['eval_rougeL']))\n",
    "xlingual_rougeL = pd.DataFrame(rougeL)\n",
    "xlingual_rougeL.index = xlingual_exp_names_1 + xlingual_exp_names_3\n",
    "xlingual_exact_match = pd.DataFrame(exact_match)\n",
    "xlingual_exact_match.index = xlingual_exp_names_1 + xlingual_exp_names_3\n",
    "\n",
    "\n",
    "def generate_new_columns(columns):\n",
    "    new_columns = []\n",
    "    for c in columns:\n",
    "        new_column = []\n",
    "        for first in c.split('_'):\n",
    "            new_column.append(first[0].upper())\n",
    "        new_columns.append(\"\".join(new_column))\n",
    "    return new_columns\n",
    "xlingual_exact_match.columns = generate_new_columns(xlingual_exact_match.columns)\n",
    "xlingual_rougeL.columns = generate_new_columns(xlingual_rougeL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "{} &         AC &       CEC &        TE &        TG \\\\\n",
      "\\midrule\n",
      "xlingual\\_original             &   76.3±3.4 &  43.1±0.7 &  19.7±1.4 &  10.3±2.0 \\\\\n",
      "xlingual\\_delete\\_stopwords     &   69.0±4.3 &  20.8±0.6 &  10.1±0.2 &   5.7±2.2 \\\\\n",
      "xlingual\\_delete\\_10\\_words      &  56.3±20.8 &  28.9±2.2 &  11.2±4.9 &   9.7±1.6 \\\\\n",
      "xlingual\\_insert\\_10\\_words      &   72.0±2.2 &  35.5±1.4 &  17.6±3.4 &   9.5±0.6 \\\\\n",
      "xlingual\\_replace\\_10\\_words     &   68.3±0.5 &  39.7±1.5 &  17.8±3.1 &  10.5±1.6 \\\\\n",
      "xlingual\\_shuffle\\_words        &    3.0±1.6 &   0.2±0.1 &   1.4±2.0 &   8.6±2.3 \\\\\n",
      "xlingual\\_repeat\\_sentences     &   73.3±2.6 &  32.6±2.7 &  14.6±3.5 &   9.2±1.7 \\\\\n",
      "xlingual\\_shuffle\\_sentences    &   69.7±3.1 &  34.8±1.7 &  17.3±0.9 &   5.8±1.7 \\\\\n",
      "xlingual\\_shuffle\\_instructions &    0.0±0.0 &  14.9±1.7 &   0.0±0.0 &   0.6±0.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61464/3841236022.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(pd.concat([xlingual_exact_match.iloc[:, :-1], xlingual_rougeL.iloc[:, :-1]], axis=1).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([xlingual_exact_match.iloc[:, :-1], xlingual_rougeL.iloc[:, :-1]], axis=1).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &         AC &       CEC &        TE \\\\\n",
      "\\midrule\n",
      "xlingual\\_original             &   76.3±3.4 &  43.1±0.7 &  19.7±1.4 \\\\\n",
      "xlingual\\_delete\\_stopwords     &   69.0±4.3 &  20.8±0.6 &  10.1±0.2 \\\\\n",
      "xlingual\\_delete\\_10\\_words      &  56.3±20.8 &  28.9±2.2 &  11.2±4.9 \\\\\n",
      "xlingual\\_insert\\_10\\_words      &   72.0±2.2 &  35.5±1.4 &  17.6±3.4 \\\\\n",
      "xlingual\\_replace\\_10\\_words     &   68.3±0.5 &  39.7±1.5 &  17.8±3.1 \\\\\n",
      "xlingual\\_shuffle\\_words        &    3.0±1.6 &   0.2±0.1 &   1.4±2.0 \\\\\n",
      "xlingual\\_repeat\\_sentences     &   73.3±2.6 &  32.6±2.7 &  14.6±3.5 \\\\\n",
      "xlingual\\_shuffle\\_sentences    &   69.7±3.1 &  34.8±1.7 &  17.3±0.9 \\\\\n",
      "xlingual\\_shuffle\\_instructions &    0.0±0.0 &  14.9±1.7 &   0.0±0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46235/2672335181.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(xlingual_exact_match.iloc[:, :-1].to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(xlingual_exact_match.iloc[:, :-1].to_latex())\n",
    "# print(xlingual_rougeL.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'english_exact_match': english_exact_match, 'english_rougeL': english_rougeL, \\\n",
    "    'xlingual_exact_match': xlingual_exact_match, 'xlingual_rougeL': xlingual_rougeL}\n",
    "for metric_name, metric_var in metrics.items():\n",
    "    metric_var.to_csv(metric_name+'_means.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "english_exp_names_1 = []\n",
    "english_exp_names_3 = ['english_new_instruction']\n",
    "\n",
    "exact_match = collections.defaultdict(list)\n",
    "rougeL = collections.defaultdict(list)\n",
    "classifications = ['answerability_classification', 'cause_effect_classification', 'coreference_resolution', \\\n",
    "    'dialogue_act_recognition', 'word_analogy', 'textual_entailment']\n",
    "non_classifications = ['data_to_text', 'grammar_error_correction', 'title_generation',\\\n",
    "    'keyword_tagging', 'overlap_extraction', 'question_rewriting']\n",
    "for english_exp_name in english_exp_names_1:\n",
    "    metric_file = open('./tk_outputs/output_{}/all_results.json'.format(english_exp_name), 'r')\n",
    "    metric = json.load(metric_file)\n",
    "\n",
    "\n",
    "    for classification in classifications:\n",
    "        exact_match[classification].append(metric['eval_exact_match_for_{}'.format(classification)])\n",
    "    exact_match['eval_exact_match'].append(metric['eval_exact_match'])\n",
    "    for non_classification in non_classifications:\n",
    "        rougeL[non_classification].append(metric['eval_rougeL_for_{}'.format(non_classification)])\n",
    "    rougeL['eval_rougeL'].append(metric['eval_rougeL'])\n",
    "\n",
    "metric_file = open('./tk_outputs/output_{}/all_results.json'.format(\"english_original_instruction\"), 'r')\n",
    "metric = json.load(metric_file)\n",
    "metric_names = metric.keys()\n",
    "# print(metric)\n",
    "def create_dict(keys):\n",
    "    d = {}\n",
    "    for key in keys:\n",
    "        d[key] = []\n",
    "    return d    \n",
    "for english_exp_name in english_exp_names_3:\n",
    "    metric_samples = create_dict(metric_names)\n",
    "    metric_means = create_dict(metric_names)\n",
    "    metric_stds = create_dict(metric_names)\n",
    "    for index in [4,5,6]:\n",
    "        _english_exp_name = english_exp_name + \"_{}\".format(index)\n",
    "\n",
    "        metric_file = open('./tk_outputs/output_{}/all_results.json'.format(_english_exp_name), 'r')\n",
    "        metric = json.load(metric_file)\n",
    "        for metric_name in metric:\n",
    "            if metric_name.endswith(f'_new_{index-3}'):\n",
    "                metric_name_pruned = metric_name.replace(f'_new_{index-3}', '')\n",
    "            else:\n",
    "                metric_name_pruned = metric_name\n",
    "            metric_samples[metric_name_pruned].append(metric[metric_name])\n",
    "    \n",
    "    for metric_name in metric_names:\n",
    "        metric_means[metric_name] = round(np.mean(metric_samples[metric_name]), 1)\n",
    "    for metric_name in metric_names:\n",
    "        metric_stds[metric_name] = round(np.std(metric_samples[metric_name]), 1)\n",
    "\n",
    "    # metrics = metric_means\n",
    "\n",
    "    for classification in classifications:\n",
    "        metric_mean = metric_means['eval_exact_match_for_{}'.format(classification)]\n",
    "        metric_std = metric_stds['eval_exact_match_for_{}'.format(classification)]\n",
    "        exact_match[classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    \n",
    "    for non_classification in non_classifications:\n",
    "        metric_mean = metric_means['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        metric_std = metric_stds['eval_rougeL_for_{}'.format(non_classification)]\n",
    "        rougeL[non_classification].append(str(metric_mean) + \"\\u00B1\" + str(metric_std))\n",
    "    exact_match['eval_exact_match'].append(str(metric_means['eval_exact_match']) + \"\\u00B1\" + str(metric_stds['eval_exact_match']))\n",
    "    rougeL['eval_rougeL'].append(str(metric_means['eval_rougeL']) + \"\\u00B1\" + str(metric_stds['eval_rougeL']))\n",
    "english_rougeL = pd.DataFrame(rougeL)\n",
    "english_rougeL.index = english_exp_names_1 + english_exp_names_3\n",
    "english_exact_match = pd.DataFrame(exact_match)\n",
    "english_exact_match.index = english_exp_names_1 + english_exp_names_3\n",
    "\n",
    "def generate_new_columns(columns):\n",
    "    new_columns = []\n",
    "    for c in columns:\n",
    "        new_column = []\n",
    "        for first in c.split('_'):\n",
    "            new_column.append(first[0].upper())\n",
    "        new_columns.append(\"\".join(new_column))\n",
    "    return new_columns\n",
    "english_exact_match.columns = generate_new_columns(english_exact_match.columns)\n",
    "english_rougeL.columns = generate_new_columns(english_rougeL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &        AC &       CEC &        CR &       DAR &        WA &        TE &       EEM \\\\\n",
      "\\midrule\n",
      "english\\_new\\_instruction &  65.8±2.2 &  67.3±2.9 &  53.0±1.9 &  44.5±7.0 &  42.3±3.1 &  49.3±4.9 &  36.5±1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       DTT &       GEC &        TG &        KT &        OE &        QR &        ER \\\\\n",
      "\\midrule\n",
      "english\\_new\\_instruction &  43.6±1.1 &  87.6±1.1 &  23.8±0.9 &  70.7±0.3 &  35.1±3.1 &  80.9±0.2 &  57.6±0.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23280/2079488218.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(english_exact_match.to_latex())\n",
      "/tmp/ipykernel_23280/2079488218.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(english_rougeL.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(english_exact_match.to_latex())\n",
    "print(english_rougeL.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{tabular}{llllllll}\n",
    "\\toprule\n",
    "{} &        AC &       CEC &        CR &       DAR &        WA &        TE &       EEM \\\\\n",
    "\\midrule\n",
    "english\\_original\\_instruction &  69.5±1.9 &  69.2±0.6 &  58.5±2.1 &  48.7±3.1 &  44.5±0.4 &  44.8±2.1 &  37.4±0.5 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "\\begin{tabular}{llllllll}\n",
    "\\toprule\n",
    "{} &       DTT &       GEC &        TG &        KT &        OE &        QR &        ER \\\\\n",
    "\\midrule\n",
    "english\\_original\\_instruction &  44.7±0.6 &  85.0±1.2 &  25.5±1.1 &  73.2±0.2 &  34.7±1.9 &  81.0±0.4 &  58.7±0.7 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test tk_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a am student I'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def random_shuffle(Definition):\n",
    "\n",
    "    token_list = Definition.split()\n",
    "    random.shuffle(token_list)\n",
    "    return \" \".join(token_list)\n",
    "random_shuffle(\"I am a student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/tk-instruct-3b-def\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/tk-instruct-3b-def\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "        # \"Definition: return the currency of the country. Now complete the following example - Input: China. Output:\", \n",
    "        \"فتوسنتز فرایندی زیست‌شیمیایی است که در آن، انرژی نورانی خورشید توسط گیاهان و برخی از باکتری‌ها به انرژی شیمیایی ذخیره‌شده در مواد غذایی آن‌ها تبدیل می‌شود. کمابیش همهٔ جانداران روی زمین به آن وابسته‌اند. در عمل فتوسنتز، اندام‌هایی مانند برگ که دارای سبزینه هستند، کربن دی‌اکسید، آب و نور را جذب کرده و به کلروپلاست می‌رسانند. طی واکنش‌هایی که درون کلروپلاست انجام می‌گیرد، این مواد به اکسیژن و کربوهیدرات‌ها تبدیل می‌شوند. همه اکسیژن کنونی موجود بر روی زمین، فراوردهٔ فتوسنتز است. برخی از کربوهیدرات‌های مهم تولیدشده مانند گلوکز، می‌توانند به دیگر مواد آلی، لیپیدها، نشاسته، سلولز و پروتئین تبدیل شوند که برای تبدیل‌شدن به پروتئین، نیاز به نیتروژن دارند. ژان باپتیست ون هلمونت، یکی از نخستین آزمایش‌های مربوط به فتوسنتز را انجام داد. همه بخش‌های سبزرنگ گیاه، قادر به انجام عمل فتوسنتز هستند. مادهٔ سبز موجود در گیاهان که سبزینه یا کلروفیل نام دارد، آغازکنندهٔ واکنش‌های فتوسنتز است. فتوسنتز در اندام‌هایی که فاقد سبزینه هستند، انجام نمی‌گیرد.\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "output = model.generate(input_ids, max_length=10)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load ni_dataset_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ef21323b704ac40a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset natural_instructions/default to ../cache/natural_instructions/default-ef21323b704ac40a/2.0.0/241d69c989a9550d22fa88048671e911038842f20f900bbc514909343345ccac...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0312d729bc472b99aa7d5918b49bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1568fceb5e284eec96ad4ae92a11401a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f587ee55194352a774029068db26cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset natural_instructions downloaded and prepared to ../cache/natural_instructions/default-ef21323b704ac40a/2.0.0/241d69c989a9550d22fa88048671e911038842f20f900bbc514909343345ccac. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3349afa049904db8b8732bbfc4b9fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Task': \"['translation', 'en-es']\", 'Contributors': 'instruction_induction', 'Source': ['instruction_induction'], 'URL': ['instruction_induction'], 'Categories': ['instruction_induction'], 'Reasoning': ['instruction_induction'], 'Definition': ['Translate to Spanish'], 'Positive Examples': [], 'Negative Examples': [], 'Input_language': ['English'], 'Output_language': ['English'], 'Instruction_language': ['English'], 'Domains': ['instruction_induction'], 'Instance': {'input': 'ile', 'output': ['montón']}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "raw_datasets = load_dataset(\n",
    "    \"../src/induction_dataset.py\", \n",
    "    data_dir='../induction_data/', \n",
    "    task_dir='../induction_data/tasks/', \n",
    "    cache_dir='../cache/',\n",
    "    max_num_instances_per_task=100,\n",
    "    max_num_instances_per_eval_task=100,\n",
    "    download_mode = 'reuse_cache_if_exists'\n",
    ")\n",
    "print(raw_datasets['test'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e82d9b055b27d6b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset natural_instructions/default (download: Unknown size, generated: 55.96 MiB, post-processed: Unknown size, total: 55.96 MiB) to /home/xxx/.cache/huggingface/datasets/natural_instructions/default-8e82d9b055b27d6b/2.0.0/c0040095172c0d76abd173b8640d8a5c5293cbc538c41e16bb88bdb4d0bb6a46...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e86ff2749846c4a1baa42cefa15fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea3565a5874bf7907530ab88c0bd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13d833adfc942ce929e60edef907cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset natural_instructions downloaded and prepared to /home/xxx/.cache/huggingface/datasets/natural_instructions/default-8e82d9b055b27d6b/2.0.0/c0040095172c0d76abd173b8640d8a5c5293cbc538c41e16bb88bdb4d0bb6a46. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c3430b2b3e4cabbd8770335ff3d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Read the passage and find the corresponding pronoun for the given name. The pronoun should match the given blank(_). The word between ** ** is the target name. The pronoun should be one of 'her', 'him', 'he', 'she' and 'his' with proper casing based on the position in the passage.\"]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "raw_datasets = load_dataset(\n",
    "    \"../src/ni_dataset.py\", \n",
    "    data_dir='../data/splits/default', \n",
    "    task_dir='../data/tasks', \n",
    "    # cache_dir='../cache/',\n",
    "    max_num_instances_per_task=100,\n",
    "    max_num_instances_per_eval_task=100,\n",
    "    # download_mode = 'reuse_cache_if_exists'\n",
    "    download_mode=\"force_redownload\"\n",
    ")\n",
    "print(raw_datasets['validation'][101]['Definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ec6c0847c0c95340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset natural_instructions/default to ../cache/natural_instructions/default-ec6c0847c0c95340/2.0.0/fb2cebb5e38344990106cf3b5c18496b4bf21e03c6d6a88d149d8279e4a82da3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d548d4c4de24a15b06b8d5ca44885ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b294ca6b2b5a498685dad0357e996342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition_native:  Write a suitable title for the text you see, containing the subject of this text, preferably in fifteen words or less.\n",
      "Definition_change:  Write a subject subject for this text you see. the the subject for this text, written in fifteen words or more.\n",
      "Definition_native:  Read the passage and find its corresponding pronoun for the target name. The position of the pronoun is at the given blank (_). The word between ** ** is the target name. The pronoun should be one of \"she\", \"he\", \"him\", \"she\" and \"he\". Appropriate case is required depending on where the pronoun is located.\n",
      "Definition_change:  Read the passage and find the corresponding pronoun for the target name. The position of the pronoun is at the given blank ( right ). The position between * * * * is the target name. The pronoun should be one of \" he \", \" he \", \" him \", \" she \" and \" he \". Appropriate case is required depending on where the pronoun is located.\n",
      "Definition_native:  Determine whether the content of two given sentences clearly contains the meaning of the other (entailment), conflicts with the meaning of the other (contradiction), or the relationship between the content of the two cannot be determined (neutral). The answer must be one of the letters E(entailment), C(contradiction) and N(neutral).\n",
      "Definition_change:  Determine whether the content of the given sentences clearly contains the meaning of the other ( entailment ), conflicts with the meaning of the other ( contradiction ), or the relationship between the content of the two cannot be determined ( neutral ). The answer must be one of the letters E ( entailment ), C ( contradiction ) and N ( neutral ).\n",
      "Definition_native:  There are two sentences, a premise and a Hypothesis. From the given pair of sentences, you should determine whether there is enough information in the Premise to support the inference made in the Hypothesis. The Premise may not be exactly the same as the Hypothesis. Your task is to return \"entails\" if the premise supports the hypothesis, and \"neutral\" otherwise.\n",
      "Definition_change:  There are two sentences, a premise and a Hypothesis. From the given pair of sentences, you should determine whether there is enough information in the Premise to support the inference made in the Hypothesis. The Premise may be to be the same as the Hypothesis. Your task is to be \" entails \" if the premise supports the hypothesis, and \" neutral \" otherwise.\n",
      "Definition_native:  Your task is to read a question with a vague description (there can be more than one answer to this question). Write a new question, one that contains information from the given problem but has only one unique answer.\n",
      "Definition_change:  Your task is to answer a question with a vague description ( there can be more than one answer to a question ). For a new question, the that contains all of the given problem, has only one unique answer.\n",
      "Definition_native:  You are given a premise sentence, two possible options \"A\" or \"B\" and a question word. If the question word is \"cause\", you should choose the option that could be the cause of the premise sentence; if the question is \"effect\", you should find the option that could be the effect of the premise sentence. You should only return \"A\" or \"B\". \n",
      "Definition_change:  You are given a premise sentence, two possible options \" A \" or \" B \" and a question word. If the question word is \" cause \", you should choose the option that could be the cause of the premise sentence ; if the question is \" effect \", you should find the option that could be the effect of the premise sentence. You you word return \" A \" or \" B \".\n",
      "Definition_native:  Your task is to read an article and generate a title for this article.\n",
      "Definition_change:  is is is to read an is and is is... is.\n",
      "Definition_native:  In this task, you will be given a sentence. You need to identify the coreference words from it. Coreference words are a set of words in the text that represents the same task or object. You need to make sure that you write the coreference words that appear in the sentence and do not use any external knowledge to express the coreference words. If there is a \"the\" before a person or object, you should include it in the coreference words. Please output the coreference words separated by commas.\n",
      "Definition_change:  In a task, you will be given a sentence. You need to identify the coreference words from it. Coreference words are a set of words in the sentence that represents the same task or object. You need to make sure that you write the coreference words that appear in the sentence and do not have any external knowledge to express the coreference words. If there is a \" the \" before a person or object, you should include it in the coreference words. Please note the two words separated by commas.\n",
      "Definition_native:  You will be given a sentence of a conversation between a human and a virtual assistant. Your task is to assign the sentence to one of the following five action categories - INFORM, INFORM_INTENT, OFFER, REQUEST, REQUEST_ALTS. If the sentence is about informing something, generate 'INFORM'. If it is about requesting something, generate 'REQUEST'. 'REQUEST_ALTS' is generated if the content of the sentence is a request for another choice instead of the option provided. Similarly, generate 'OFFER' if the sentence is offering some information. If the sentence is about showing intent to do something, generate 'INFORM_INTENT'.\n",
      "Definition_change:  You will be given a sentence of a conversation between a human and a virtual assistant. Your task is to assign the sentence to one of the following five action categories - INFORM, INFORM _ INTENT, OFFER, REQUEST, AND _ ALTS. If the sentence is about informing something, generate'INFORM '. If it is about doing something, generate'REQUEST '.'F _ ALTS'is generated if the content of the sentence is a request for another choice instead of the option provided. Similarly, generate'OFFER'if the sentence is offering some information. If the sentence is about showing intent to do something, generate'INFORM _ INTENT '.\n",
      "Definition_native:  Given a passage about cooking, and a set of question-and-answer dialogues about the passage, your task is to determine if the passage contains enough information to answer the follow-up questions. Say `Yes' if it can be answered, otherwise say `No'. Each conversational question is prefixed with `Q:`, followed by the answer prefix `A:`, and the follow-up question is prefixed with `FOLLOWUP_Q:`.\n",
      "Definition_change:  With a passage about cooking, and a set of answer - and - answer dialogues about the passage, your task is to determine if the passage has enough information to answer the follow - up questions. Say No'if it can be answered, otherwise say No '. Each conversational question is prefixed with Q :, followed by the prefix A :, and the follow - up question is prefixed with FOLLOWUP _ Q :.\n",
      "Definition_native:  You are given a sentence and your task is to produce a topic word. The topic word is the core of a sentence's focus; it is usually the name of a person, place, animal, concept or object.\n",
      "Definition_change:  You are given a sentence and your task is to produce the topic word. The topic word is the word of a person's knowledge ; it is also the language of a person, place, animal, concept or object.\n",
      "Definition_native:  You will get one or more triplets. Elements are separated by '|'. The second part of each triplet represents the relationship between the first and third elements. Your task is to write a simple and short sentence or paragraph describing these triplets in natural language.\n",
      "Definition_change:  You will get one or more triplets. Elements are separated by'| '. The second part of each triplet represents the difference between the first and the elements. Your task is to write a simple and simple sentence or paragraph. these triplets in natural language.\n",
      "Definition_native:  You will be given a Wikipedia article on a topic and a question related to that topic. Please determine if the question is answerable from this article. If the question is answerable, please answer \"TRUE\", otherwise please answer \"FALSE\".\n",
      "Definition_change:  You must be given a Wikipedia article on this topic and a question related to that topic. Please determine if the question is related from this topic. If the question is answerable, you answer \" TRUE \", otherwise please answer \" FALSE \".\n",
      "Definition_native:  Given an abstract, your task is to generate a noun phrase that describes the focus or contribution of the paper. These keywords can come directly from the given abstract or can be outside the abstract.\n",
      "Definition_change:  Given an abstract, your task is to generate a nounword that is the focus or contribution of the paper. These keywords can come directly from a given abstract or can be from the paper.\n",
      "Definition_native:  You will be given two analogies that relate items to related containers, and they will be in the format \"A : B. C : ?\" . \"Your task is to replace the question mark (?) with the appropriate container according to the \"A : B\" relationship to represent the given item C.\n",
      "Definition_change:  There will be given two analogies that represent items to related containers, and they will be in the case \" A. B. C :? \". \" Your task is to replace the question mark ( item ) with the appropriate container according to the \" A - B \" relationship to represent the given item to \"\n",
      "Definition_native:  Given two sentences, there are multiple overlapping words between them. Your task is to extract one of them. The overlapping words do not need to be exactly the same, for example, \"survival\" and \"survival\" are valid overlapping words. You must produce important words, rather than stop words like \"the\" or \"of\".\n",
      "Definition_change:  Given the sentences, there are multiple overlapping words between them. Your task is to extract one of them. The other words do not need to be exactly the same, for example, \" survival \" and \" survival \" are valid overlapping words. You must stop important words, rather than stop words like \" of \" or \" of \".\n",
      "Definition_native:  Two analogies are given, each of which corresponds a location or place to a vehicle in the format \"A : B. ? C : ?\" . Your task is to replace the question mark (?) with the corresponding mode of travel according to the given location C, following the \"A : B\" relationship .\n",
      "Definition_change:  Two analogies are given, each of which corresponds a location or place of a vehicle in the format \" A : B.? C :? \". Your task is to replace the first mark (? ) with the \" mark of travel according to the given location mark, following the \" A : B \" relationship.\n",
      "Definition_native:  Given a piece of data in table format, your task is to generate a plain text paragraph describing the information in the table.\n",
      "Definition_change:  For the implementation of writing in table format, your function is to generate a plain text file describing the text in the format.\n",
      "Definition_native:  You have been given a fill-in-the-blank question for which the answer is already known to be PersonX. You need to make the answer to the given question PersonY with as little change as possible. This task usually involves replacing a word with an antonym, which can be called a \"trigger word\" (for example, changing \"sympathetic\" to \"stern\"). You should not change anything in the given question except one or two trigger words/phrases. For your question, PersonY must be significantly more likely to fill in the blanks than PersonX. The questions you write should NOT contain potentially explicit, offensive, or adult content. Do not use real people's names or generic names in your questions (e.g., Donald Trump, John Doe, etc.). When writing your questions, avoid repeating the same style or phrases and try to add variety to your words. For example, this task can always be solved by adding simple negatives such as not, never, etc. Your question must contain a minimum of 15 and a maximum of 30 words. Your question must have at least 70% overlap words with the given question. You must use the given contextual words when writing your question. Your question must contain only one blank. Make sure PersonX and PersonY have the same gender. PersonX and PersonY should be used only ONCE in your question, and PersonX should appear before PersonY. Although there are many correct answers, you only need to write one of them.\n",
      "Definition_change:  You have been given a fill - in - the - blank question for which the answer is already known to be PersonX. You should not make the answer to the given question, with as little change as possible. This task usually involves replacing a word with an antonym, which can be called a \" trigger word \" ( for example, changing \" sympathetic \" to \" stern \" ). You should not change anything in the given question except one or two trigger words / phrases. For your question, PersonY must be much more likely to fill in the blanks than PersonX. The questions you write should NOT contain potentially explicit, offensive, or adult content. Do not use real people's names or generic names in your questions ( e. g., Donald Trump, John Doe, etc. ). When writing your questions, avoid repeating the same style or phrases and try to add variety to your words. For example, this task can always be solved by adding simple negatives such as not, no, etc. Your question must contain a minimum of 15 and a maximum of 30 words. Your question must have at least 70 % overlap words with the given question. You must use the given contextual words when writing your question. Your question must contain only one blank. Make sure PersonX and PersonY have the same gender. PersonX and PersonY should be used only ONCE in your question, and PersonX should appear before PersonY. Although there are many correct answers, you only need to write one of them.\n",
      "Definition_native:  Given two sentences separated by \", so\", your task is to decide if the first sentence can be the cause of the second sentence. Answer \"plausible\" if you can see a possible cause-effect relationship, otherwise answer \"not plausible\".\n",
      "Definition_change:  Given two sentences separated, \", so \", your task is to decide if the first sentence can be the cause of the second sentence. Answer \" plausible \" If you can see a possible cause - effect relationship, and answer \" not plausible \".\n",
      "Definition_native:  Given a sentence as input, your task is to predict the type of the sentence. Your prediction can be one of these: 'information', if the input sentence states a piece of information; 'question', if the input sentence asks for some information; 'directive', if the input sentence involves management or guidance on behavior; 'commissive', if the input sentence is a speaker promising some future action.\n",
      "Definition_change:  Given a sentence as input, your task is to predict the type of the sentence. Your prediction can be one of these :'positive ', if the input sentence states a piece of information ;'positive ', if the input sentence asks for some information ;'negative ', if the input sentence involves management or based on behavior ;'commissive ', if the input sentence is a speaker promising some future action.\n",
      "Definition_native:  You will be given three sentences. Read these sentences and then identify a noun phrase (person, place, thing) or event that is present in all three sentences. Your task is to find the text in each sentence that corresponds to that phrase. Your answers should keep the order of the sentences like this. 1: corresponding phrase of sentence 1, 2: corresponding phrase of sentence 2, 3: corresponding phrase of sentence 3\n",
      "Definition_change:  It should be given three sentences. Read these sentences and then identify a noun phrase ( person, place, thing ) or event that is present in all three sentences. Your task is to find the text in each sentence that corresponds to that phrase. Your answers should keep the order of the sentences like this. 1 : corresponding phrase of sentence 1, 3 : corresponding phrase of sentence 1, 3 : corresponding phrase of sentence 1\n",
      "Definition_native:  Given an incorrect English sentence. Your task is to correct the input sentence and write out his correct form.\n",
      "Definition_change:  for an incorrect English sentence. Your language is to to the inputs and write out the correct form.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a15537842254a3a83458c9274033460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset natural_instructions downloaded and prepared to ../cache/natural_instructions/default-ec6c0847c0c95340/2.0.0/fb2cebb5e38344990106cf3b5c18496b4bf21e03c6d6a88d149d8279e4a82da3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae87add0f745428e838ff151d7eba249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "raw_datasets = load_dataset(\n",
    "    \"../src/ni_dataset_perturb.py\", \n",
    "    data_dir='../data/splits/default', \n",
    "    task_dir='../data/tasks', \n",
    "    cache_dir='../cache/',\n",
    "    max_num_instances_per_task=100,\n",
    "    max_num_instances_per_eval_task=100,\n",
    "    perturb_method='replace_words',\n",
    "    download_mode = 'reuse_cache_if_exists'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['In this task your given two statements in Estonian. You must judge whether the second sentence is the cause or effect of the first one. Label the instances as \"cause\" or \"effect\" based on your judgment. The sentences are separated by a newline character.']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPerturbation:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.en = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    @staticmethod\n",
    "    def connect_token_segments(tokens):\n",
    "        connected_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                connected_tokens[-1] = connected_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                connected_tokens.append(token)\n",
    "        return connected_tokens\n",
    "\n",
    "\n",
    "    def delete_words(self, Definition, num=5):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(Definition)\n",
    "        tokens = self.connect_token_segments(tokens)\n",
    "\n",
    "        index = [i for i in range(len(tokens))]\n",
    "\n",
    "        deleted_index = random.sample(index, num)\n",
    "        deleted_index = set(deleted_index)\n",
    "\n",
    "        deleted_tokens = [tokens[i] for i in index if i not in deleted_index]\n",
    "        Definition_perturb = self.tokenizer.convert_tokens_to_string(deleted_tokens)\n",
    "        return Definition_perturb\n",
    "        \n",
    "    def delete_stopwords(self, Definition):\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(Definition)\n",
    "        tokens = self.connect_token_segments(tokens)\n",
    "\n",
    "        stopwords = self.en.Defaults.stop_words\n",
    "        deleted_tokens=[]\n",
    "        for token in tokens:\n",
    "            if token.lower() not in stopwords:\n",
    "                deleted_tokens.append(token)\n",
    "        Definition_perturb =  self.tokenizer.convert_tokens_to_string(deleted_tokens)\n",
    "        return Definition_perturb\n",
    "\n",
    "    def insert_words(self, Definition, num_mask=5):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(Definition)\n",
    "        if len(tokens)>512:\n",
    "            return Definition\n",
    "        tokens = self.connect_token_segments(tokens)\n",
    "\n",
    "        index = [i for i in range(len(tokens))]\n",
    "\n",
    "        index = random.sample(index, num_mask)\n",
    "\n",
    "        for i in index:\n",
    "            tokens.insert(i, '[MASK]')\n",
    "\n",
    "        \n",
    "        \n",
    "        Definition = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "        inputs = self.tokenizer(Definition, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "        _, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "\n",
    "        predicted_index = [sorted_idx[i, 0].item() for i in range(0, len(predictions[0])-1)]\n",
    "        for x in range(1, len(predictions[0])-1):\n",
    "            if input_ids[x] == 103:\n",
    "                input_ids[x] = predicted_index[x]\n",
    "\n",
    "        return self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def replace_words(self, Definition, num_mask=5):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(Definition)\n",
    "        if len(tokens)>512:\n",
    "            return Definition\n",
    "        tokens = self.connect_token_segments(tokens)\n",
    "\n",
    "        index = [i for i in range(len(tokens))]\n",
    "\n",
    "        index = random.sample(index, num_mask)\n",
    "\n",
    "        for i in index:\n",
    "            tokens[i] = '[MASK]'\n",
    "\n",
    "        if len(tokens)>512:\n",
    "            return Definition\n",
    "        \n",
    "        Definition = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "        inputs = self.tokenizer(Definition, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "        _, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "\n",
    "        predicted_index = [sorted_idx[i, 0].item() for i in range(0, len(predictions[0])-1)]\n",
    "        for x in range(1, len(predictions[0])-1):\n",
    "            if input_ids[x] == 103:\n",
    "                input_ids[x] = predicted_index[x]\n",
    "\n",
    "        return self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def shuffle_words(self, Definition):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(Definition)\n",
    "        tokens = self.connect_token_segments(tokens)\n",
    "\n",
    "        random.shuffle(tokens)\n",
    "        return self.tokenizer.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    def shuffle_sentences(self, Definition):\n",
    "\n",
    "        doc = self.en(Definition)\n",
    "        sents = list(map(str, doc.sents))\n",
    "        random.shuffle(sents)\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    def repeat_sentences(self, Definition, index = None):\n",
    "        doc = self.en(Definition)\n",
    "        sents = list(map(str, doc.sents))\n",
    "        if None == index:\n",
    "            index = random.randint(0, len(sents)-1)\n",
    "        sents = sents[:index] + [sents[index]] + sents[index:]\n",
    "        return \" \".join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick fox jumps over the lazy dog .\n",
      "quick brown fox jumps lazy dog .\n",
      "The quick brown - fox \" jumps over the lazy dog.\n",
      "The quick dog fox jumps over the big dog.\n",
      ". The lazy dog fox jumps brown the quick over\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "data_perturbation = DataPerturbation()\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(data_perturbation.delete_words(sentence, 2))\n",
    "print(data_perturbation.delete_stopwords(sentence))\n",
    "print(data_perturbation.insert_words(sentence, 2))\n",
    "print(data_perturbation.replace_words(sentence, 2))\n",
    "print(data_perturbation.shuffle_words(sentence))\n",
    "print(data_perturbation.shuffle_sentences(sentence))\n",
    "print(data_perturbation.repeat_sentences(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0  \\\n",
      "0  Original Instruction   \n",
      "1      delete_stopwords   \n",
      "2          delete_words   \n",
      "3          insert_words   \n",
      "4      repeat_sentences   \n",
      "5         replace_words   \n",
      "6     shuffle_sentences   \n",
      "7         shuffle_words   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1  \n",
      "0                                                                   You are given a sentence from a conversation between a human and a virtual assistant. Your task is to classify the sentence into one of the following five action categories - INFORM, INFORM_INTENT, OFFER, REQUEST, REQUEST_ALTS. If the sentence is about informing something, generate 'INFORM'. If it is about requesting something, generate 'REQUEST'. If the sentence mentions requesting an alternative option than the one provided, generate 'REQUEST_ALTS'. Similarly, generate 'OFFER' if the sentence is offering some information. If the sentence is about showing intent to do something, generate 'INFORM_INTENT'.  \n",
      "1                                                                                                                                                                                                                               given sentence conversation human virtual assistant . task classify sentence following action categories - INFORM , INFORM _ INTENT , OFFER , REQUEST , REQUEST _ ALTS . sentence informing , generate ' INFORM ' . requesting , generate ' REQUEST ' . sentence mentions requesting alternative option provided , generate ' REQUEST _ ALTS ' . Similarly , generate ' OFFER ' sentence offering information . sentence showing intent , generate ' INFORM _ INTENT ' .  \n",
      "2                                                                   You are given a sentence from a conversation between a human and a virtual assistant . Your task is to classify the sentence into one of following five action categories - INFORM , INFORM _ INTENT , OFFER , REQUEST , REQUEST _ ALTS If the sentence is about informing something , generate ' INFORM ' . If it is about requesting something , generate ' REQUEST ' . If the sentence mentions requesting an alternative option than the one provided , generate ' REQUEST _ ALTS ' . Similarly , generate ' OFFER ' if the sentence is some information . If the sentence is about showing intent to do , ' INFORM _ INTENT ' .  \n",
      "3                                        You are given a sentence from a conversation between a human and a virtual assistant. Your task is to classify the sentence into one of the following five action categories - INFORM, INFORM _ INTENT, OFFER, REQUEST, REQUEST _ ALTS. If the sentence is about informing something, generate'INFORM '. If it is about requesting something, generate'REQUEST '. If the sentence mentions requesting an alternative option other than the one that is provided, generate'REQUEST _ ALTS '. Similarly, generate'OFFER'if the sentence is offering some information. If the \" sentence is about showing intent to do something else, generate'INFORM _ INTENT '.  \n",
      "4  You are given a sentence from a conversation between a human and a virtual assistant. Your task is to classify the sentence into one of the following five action categories - INFORM, INFORM_INTENT, OFFER, REQUEST, REQUEST_ALTS. If the sentence is about informing something, generate 'INFORM'. If the sentence is about informing something, generate 'INFORM'. If it is about requesting something, generate 'REQUEST'. If the sentence mentions requesting an alternative option than the one provided, generate 'REQUEST_ALTS'. Similarly, generate 'OFFER' if the sentence is offering some information. If the sentence is about showing intent to do something, generate 'INFORM_INTENT'.  \n",
      "5                                                                    You are given a sentence from a conversation between a human and a virtual assistant. The task is to classify the sentence into one of the following five action categories - INFORM, INFORM _ INTENT, OFFER, REQUEST, REQUEST _ ALTS. If the sentence is about informing something, generate'INFORM '. If it is about requesting something, generate'REQUEST '. If the sentence mentions requesting an alternative option than the one is, generate'REQUEST _ ALTS '. Similarly, generate'OFFER'if the sentence is offering some information. If the sentence is about showing intent to do something, generate'INFORM _ INTENT '.  \n",
      "6                                                                   If it is about requesting something, generate 'REQUEST'. If the sentence is about showing intent to do something, generate 'INFORM_INTENT'. You are given a sentence from a conversation between a human and a virtual assistant. Similarly, generate 'OFFER' if the sentence is offering some information. Your task is to classify the sentence into one of the following five action categories - INFORM, INFORM_INTENT, OFFER, REQUEST, REQUEST_ALTS. If the sentence mentions requesting an alternative option than the one provided, generate 'REQUEST_ALTS'. If the sentence is about informing something, generate 'INFORM'.  \n",
      "7                                 about the a , action information sentence requesting If , is task generate _ Similarly REQUEST of into ' sentence informing sentence about something human , from showing categories INFORM between ' If virtual five to . , the a , alternative ' mentions _ to the is generate the You ' . assistant provided INTENT , is ' - generate ALTS . one option some requesting ' offering if REQUEST generate the sentence sentence an Your . do OFFER INTENT conversation are about it given . , ' following ' . REQUEST , generate than classify REQUEST If a is INFORM a one something is sentence _ ALTS , . INFORM and OFFER intent INFORM something the ' If ' the _  \n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "original_instruction = \"You are given a sentence from a conversation between a human and a virtual assistant. Your task is to classify the sentence into one of the following five action categories - INFORM, INFORM_INTENT, OFFER, REQUEST, REQUEST_ALTS. If the sentence is about informing something, generate 'INFORM'. If it is about requesting something, generate 'REQUEST'. If the sentence mentions requesting an alternative option than the one provided, generate 'REQUEST_ALTS'. Similarly, generate 'OFFER' if the sentence is offering some information. If the sentence is about showing intent to do something, generate 'INFORM_INTENT'.\"\n",
    "\n",
    "data_perturbation = DataPerturbation()\n",
    "attrs = (getattr(data_perturbation, name) for name in dir(data_perturbation))\n",
    "methods = filter(inspect.ismethod, attrs)\n",
    "compares = [[\"Original Instruction\", original_instruction]]\n",
    "for method in methods:\n",
    "    if method.__name__ != '__init__':\n",
    "        compares.append([method.__name__, method(original_instruction)])\n",
    "\n",
    "df_compares = pd.DataFrame(compares)\n",
    "# print(df_compares.style.to_latex(hrules=True))\n",
    "print(df_compares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "jsons = glob.glob('./data/tasks/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for j in jsons:\n",
    "    task_j = json.load(open(j))\n",
    "    if task_j['Instruction_language'][0] != 'English':\n",
    "        print(task_j['Instruction_language'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc2c0cd89e4ba02e191b1e7889aaaf3c8919053db7e24df61ffe975264e317e1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
